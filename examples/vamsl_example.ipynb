{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e05811-beaf-4e42-bc46-b948e97bf102",
   "metadata": {},
   "source": [
    "### VaMSL: Approximate inference of $p(G, \\Theta, C,  \\pi | D)$ for mixtures of (non)linear Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bdb043-b9d4-4778-a151-e493bdbb5291",
   "metadata": {},
   "source": [
    "VaMSL carries out approximate inference over mixtures of Bayesian networks (BN) using coordinate ascent variational inference (CAVI). We instantiate the CAVI update over BNs with the Differentiable Bayesian Structure Learning (DiBS) ([Lorch et al., 2021](https://proceedings.neurips.cc/paper/2021/hash/ca6ab34959489659f8c3776aaf1f8efd-Abstract.html)) framework and it's Stein Variational gradient descent (SVGD) ([Liu and Wang, 2016](https://proceedings.neurips.cc/paper/2016/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html)) implementation.\n",
    "\n",
    "DiBS leverages a generative model for graphs $p(G | Z)$ conditional on the continuous latent variable $Z$ to enable the use of gradient optimization methods in the (otherwise discrete) graph search accross BNs. Their SVGD implementation enables Bayesian structure learning by yielding a posterior in the BN space $p(G, \\Theta | D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49418b15-435a-490e-8fde-dbbcdbdc9e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install ../. --quiet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "key=random.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98f74d-e396-45d8-8d5f-285d553e9817",
   "metadata": {},
   "source": [
    "### Define the mixture model.\n",
    "\n",
    "First we define the mixture of the observations as well as the component BNs in the mixture model. \n",
    "\n",
    "The elements of `mixing rate` defines the proportion of the total number of observations that are generated from each component. The entries should sum up to 1. The number of components `n_components` in the mixture model is taken to be the number indicated by the mixing rates, but can be set to a different value if so desired.\n",
    "\n",
    "Each Gaussain BN has `n_vars` variables and has either a linear or nonlinear mean function depending on `struct_eq_type`. The nonlinear functions are modelled using shallow NNs. The random graph prior is either set to `er` (Erdos-Renyi) or `sf` (scale-free) graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5b263-22be-4284-8a2c-ef389863b904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "key = random.PRNGKey((seed:=124))\n",
    "\n",
    "# Data settings\n",
    "n_observations = 400 # total number of observations to generate\n",
    "ho_frac = 0.5 # fraction of observations held-out (for predicitve perfomance assessment)\n",
    "mixing_rate = jnp.array([1.0]) # mixing rate of observations (also defines number of DGPs)\n",
    "#mixing_rate = 1/(K:=5) * jnp.ones(K) # uniform mixing rates for K components\n",
    "\n",
    "# BN settings\n",
    "n_vars = 5 # number of variables in each component BN\n",
    "struct_eq_type = 'nonlinear' # BN function class: 'linear' or 'nonlinear'\n",
    "graph_type = 'sf' # Random graph structure: 'sf' (scale-free) or 'er' (Erdos-Renyi)\n",
    "\n",
    "# Derived variables\n",
    "n_components = mixing_rate.shape[0]\n",
    "linear = True if struct_eq_type == 'linear' else False\n",
    "n_obs = int(n_observations * (1-ho_frac))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec847fcc-3cd2-4a8f-a0ff-3e049fdccb66",
   "metadata": {},
   "source": [
    "### Generate ground truth BNs and synthetic data.\n",
    "\n",
    "We generate synthetic data from ground truth BNs as specified above. `x_ind` is an observation array with {n_observations, n_vars+1}, where the last column consits of ground truth indicators for the corresponding ground truth BN. The varaibles `lik`, `component_lik`, and `graph_model` define likelihood models and the grpah prior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e590ef-cc46-4001-9348-311fc26760ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.target import make_mixture_model\n",
    "from vamsl.utils import visualize_ground_truth, visualize_ground_truths\n",
    "\n",
    "key = random.PRNGKey((seed:=523))\n",
    "key, subk = random.split(key)\n",
    "# Generate data (with indicators for component assignment), ground truth BNs, likelihood models, and random graph model\n",
    "x_ind, ground_truth_graphs, ground_truth_thetas, lik, component_lik, graph_model = make_mixture_model(key=subk,\n",
    "                                                                                                      mixing_rate=mixing_rate,\n",
    "                                                                                                      n_vars=n_vars,\n",
    "                                                                                                      n_observations=n_observations,\n",
    "                                                                                                      graph_type=graph_type,\n",
    "                                                                                                      struct_eq_type=struct_eq_type,\n",
    "                                                                                                      edges_per_node=4,\n",
    "                                                                                                      obs_noise=0.1)\n",
    "visualize_ground_truths(ground_truth_graphs)\n",
    "#print(ground_truth_graphs)\n",
    "# Remove indicator vector from data\n",
    "x = x_ind[:n_obs,:n_vars] # [n_observations, n_vars]\n",
    "indicator = x_ind[:n_obs,n_vars]\n",
    "x_ho = x_ind[n_obs:,:n_vars] # [n_observations, n_vars]\n",
    "indicator_ho = x_ind[n_obs:,n_vars]\n",
    "print('Observations shape:')\n",
    "print('x: ' + str(x.shape))\n",
    "print('Indicators and counts:')\n",
    "print(jnp.unique(indicator, return_counts=True))\n",
    "if  n_observations > 0:\n",
    "    for i in range(n_components):\n",
    "        print('Node distributions for component ', i+1, ':')\n",
    "        pd.DataFrame(x[indicator==i, :]).hist(sharey=True,figsize=(4*n_vars,4),layout=(1,n_vars))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34aa45-c9ab-442b-840e-fc1d5a04fa73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "if  n_observations > 0:\n",
    "    n_components = n_components\n",
    "    random_state = seed\n",
    "\n",
    "    labels = BayesianGaussianMixture(n_components=n_components, random_state=random_state, init_params='random_from_data', verbose=1).fit_predict(x)\n",
    "    VGMM_assignments = confusion_matrix(indicator, labels)[:,linear_sum_assignment(confusion_matrix(indicator, labels), maximize=True)[1]]\n",
    "    print(\"\\nVGMM MAP clustering:\\n\", VGMM_assignments, '\\n')\n",
    "    print('Percentage missclassified: ', 100*jnp.sum(VGMM_assignments[~jnp.eye(n_components, dtype=bool)])/n_observations, '%') # mask diagonal with logical not of identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613dbe4-1b5d-4bc7-b58e-1fd5e864a620",
   "metadata": {},
   "source": [
    "### Create VaMSL.\n",
    "\n",
    "We create the VaMSL model and intilitialize it with `n_particles` randomly sampled particles latent $Z$ and $\\Theta $ particles as well as uniform distributions over responsibilities and weights. If one wants to specify specific prior responsibilities and weights, they can be fed to `initialize_posteriors` as named variables `init_q_c` and `alphas`, of shapes `(n_observations, n_components)` and `(n_components,)` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fe0cf-1ac8-4cbe-8d0e-37432c3c2566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.inference import VaMSL\n",
    "key = random.PRNGKey((seed:=789))\n",
    "\n",
    "# Create VaMSL and initialize posteriors (remove indicator vecor from dataset)\n",
    "vamsl = VaMSL(x=x, graph_model=graph_model, mixture_likelihood_model=lik, \n",
    "              component_likelihood_model=component_lik, \n",
    "              n_mixture_grad_mc_samples=1,\n",
    "              n_elicitation_grad_mc_samples=5000,\n",
    "              lamda=(100, 100, 1), # alpha_0, beta_0, schedule\n",
    "              elicitation_prior='Bin', #'conc_hard', None, 'test', 'hard', 'soft', 'DirSim'\n",
    "              parallell_computation=True,\n",
    "              stochastic_elicitation=True)\n",
    "key, subk = random.split(key)\n",
    "vamsl.initialize_posteriors(key=subk, n_components=n_components, n_particles=7, linear=linear, init_q_c=jax.nn.one_hot(indicator,n_components))\n",
    "\n",
    "print('Variational distribution (Posterior) shapes:')\n",
    "posts = vamsl.get_posteriors()\n",
    "print('q_z:     ' + str(posts[0].shape)) # [n_components, n_particls, d, l, 2]\n",
    "print('q_theta: ' + str(posts[1].shape)) if linear else print('q_theta: ' + str(len(posts[1]))) # leading dim of n_components\n",
    "print('log_q_c: ' + str(posts[2].shape)) # [n_observations, n_components]\n",
    "print('q_pi:    ' + str(posts[3].shape)) # [n_components,]\n",
    "\n",
    "key, subk = random.split(key)\n",
    "correct = random.uniform(key=subk, minval=0.5, maxval=0.55, shape=ground_truth_graphs.shape)\n",
    "incorrect = random.uniform(key=subk, minval=0.45, maxval=0.5, shape=ground_truth_graphs.shape)\n",
    "E = jnp.where(ground_truth_graphs, correct, incorrect)\n",
    "\n",
    "key, subk = random.split(key)\n",
    "# Specify prior elicitation matrix\n",
    "#vamsl.set_E((0.5*jnp.ones_like(ground_truth_graphs)).at[0,1,0].set(0.89).at[0,0,1].set(0.99)) # set default (unconstrained)\n",
    "vamsl.set_E(jnp.where(ground_truth_graphs, 0.7, 0.3), subk) # supply correct soft constraints \n",
    "#vamsl.set_E(ground_truth_graphs, subk) # supply correct hard constraints\n",
    "#jnp.save('E.npy')\n",
    "#vamsl.set_E(jnp.load('E.csv.npy'), subk)\n",
    "#vamsl.set_E(E, subk)\n",
    "visualize_ground_truths(vamsl.get_E(), graph_label='Elicitation matrix')\n",
    "[visualize_ground_truths(vamsl.get_E_particles()[k], graph_label='Elicitation particle') for k in range(n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916756d3-3eeb-441a-aa1f-6f2a8d9df1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# CAVI and SVGD vars\n",
    "n_cavi_updates, steps, callback_every = 0, 100, 100\n",
    "\n",
    "# CAVI-loop\n",
    "for cavi_update in range(n_cavi_updates):\n",
    "    key, subk = random.split(key)\n",
    "    # Optimize q(Z, \\Theta)\n",
    "    vamsl.update_particle_posteriors(key=subk, steps=steps, callback_every=callback_every,\n",
    "                                     callback=vamsl.visualize_callback(), linear=linear)\n",
    "\n",
    "    # Update to optimal q(c) and q(\\pi)\n",
    "    vamsl.update_responsibilities_and_weights()\n",
    "    print(f'CAVI update number {cavi_update+1}/{n_cavi_updates}')\n",
    "        \n",
    "    # Print current clustering performance\n",
    "    order = vamsl.identify_MAP_classification_ordering(ground_truth_indicator=indicator)\n",
    "    y_pred = [order[k] for k in jnp.argmax(vamsl.get_posteriors()[2], axis=1)]\n",
    "    print('MAP clustering: \\n', confusion_matrix(indicator, y_pred))\n",
    "    \n",
    "# Final CAVI update with more SVGD steps to ensure annealing unto acyclic graphs\n",
    "for long_CAVI in range(1):\n",
    "    key, subk = random.split(key)\n",
    "    vamsl.update_particle_posteriors(key=subk, steps=2000, callback_every=400, callback=vamsl.visualize_callback(), linear=linear)\n",
    "    #vamsl.update_responsibilities_and_weights()\n",
    "    \n",
    "    # Print current clustering performance\n",
    "    order = vamsl.identify_MAP_classification_ordering(ground_truth_indicator=indicator)\n",
    "    y_pred = [order[k] for k in jnp.argmax(vamsl.get_posteriors()[2], axis=1)]\n",
    "    print('MAP clustering: \\n', confusion_matrix(indicator, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5047901-675d-4fde-98c2-98ed805d1a8d",
   "metadata": {},
   "source": [
    "### Get order of components with respect to ground truths\n",
    "\n",
    "We identify an ordering of the components with respect to the ground truth BNs by identifying which perumutation of indeces results in the highest classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f1187-a5d7-49b2-a598-2ab6ab762905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute optimal ordering with respect to MAP classification accuracy\n",
    "order = vamsl.identify_MAP_classification_ordering(ground_truth_indicator=indicator)\n",
    "print('Optimal order:')\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c758de4-0ab2-4208-bd54-6a90f1d85291",
   "metadata": {},
   "source": [
    "### Evaluate classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1df7c-43f6-4c3c-ba8f-455010b638a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('Sums of responsibilities:')\n",
    "print(jnp.sum(jnp.exp(vamsl.get_posteriors()[2]), axis=0))\n",
    "print('Average entropy of responsibilities:')\n",
    "print(jnp.sum(jnp.exp(vamsl.get_posteriors()[2])*vamsl.get_posteriors()[2])/vamsl.get_posteriors()[2].shape[0])\n",
    "y_true = indicator\n",
    "y_pred = [order[k] for k in jnp.argmax(vamsl.get_posteriors()[2], axis=1)]\n",
    "if n_components==2:\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred, target_names=['Component 1', 'Component 2']))\n",
    "print('Ordered confusion matrix:\\n', (cm:=confusion_matrix(y_true, y_pred)))\n",
    "print('Percentage missclassified: ', 100*jnp.sum(cm[~jnp.eye(n_components, dtype=bool)])/n_obs, '%') # mask diagonal with logical not of identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87070f-e94b-4981-98c0-10208338b031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.metrics import ordered_MAP_classification_accuracy\n",
    "\n",
    "ho_MAP_preds = jnp.argmax(vamsl.compute_log_responsibilities(x=x_ho), axis=1)\n",
    "ordered_MAP_classification_accuracy(indicator=indicator_ho, \n",
    "                                    order=order,\n",
    "                                    MAP_preds=ho_MAP_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978b293-d51f-475f-9664-cf16bab16878",
   "metadata": {},
   "source": [
    "### Evaluate structure learning performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d97c8e-1f32-4282-9798-4ef8dc8451c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.metrics import expected_shd, threshold_metrics\n",
    "\n",
    "# Loop over components and calculate metrics\n",
    "for k, dist in zip(range(n_components), vamsl.get_component_dists()):\n",
    "    # Calculate metrics\n",
    "    eshd = expected_shd(dist=dist, g=ground_truth_graphs[order[k]])       \n",
    "    auroc = threshold_metrics(dist=dist, g=ground_truth_graphs[order[k]])['roc_auc']\n",
    "    \n",
    "    print(f' Component {k+1:4d} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b3ee1-b081-42ca-a1a3-27820e289d26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate predicitive performance on held-out observations\n",
    "\n",
    "We use the MAP predicted assignments of held-out observations to each component.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab238a44-9108-422f-b0c9-a8c498703099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.metrics import MAP_assigned_neg_ave_log_lik\n",
    "\n",
    "MAP_assigned_neg_ave_log_lik(x=x_ho,\n",
    "                             MAP_assignments=jnp.argmax(vamsl.compute_log_responsibilities(x=x_ho), axis=1), \n",
    "                             dists=vamsl.get_component_dists(), \n",
    "                             eltwise_log_likelihood=vamsl.eltwise_component_log_likelihood_observ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96337422-1b2a-4de6-a672-a497452bf558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.metrics import expected_log_mixture_lik\n",
    "\n",
    "key, subk = random.split(key)\n",
    "expected_log_mixture_lik(key=subk,\n",
    "                         q_pi=vamsl.get_posteriors()[3],\n",
    "                         dists=vamsl.get_component_dists(),\n",
    "                         eltwise_log_likelihood=vamsl.eltwise_component_log_likelihood_observ,\n",
    "                         x=x_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83edc82-8023-49e3-8dbd-d732ef43dddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.metrics import MAP_assigned_lppd\n",
    "\n",
    "MAP_assigned_lppd(x=x_ho,\n",
    "                  dists=vamsl.get_component_dists(),\n",
    "                  MAP_assignments=jnp.argmax(vamsl.compute_log_responsibilities(x=x_ho), axis=1),\n",
    "                  eltwise_log_likelihood=vamsl.eltwise_component_log_likelihood_observ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4641e1-1b1e-4420-8f6e-9283c46d1cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.metrics import mixture_lppd\n",
    "\n",
    "key, subk = random.split(key)\n",
    "mixture_lppd(key=subk,\n",
    "             q_pi=vamsl.get_posteriors()[3],\n",
    "             x=x_ho,\n",
    "             dists=vamsl.get_component_dists(),\n",
    "             eltwise_log_likelihood=vamsl.eltwise_component_log_likelihood_observ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3909cf-c46d-4a89-96ec-e19107126127",
   "metadata": {},
   "source": [
    "### Elicitation\n",
    "\n",
    "Incoporate elicited hard and soft edge constraints into VaMSL. Either provide an elicitation matrix, construct one by answering queries, or let a oracle simulate the process of querying an expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdc2b2-1eb6-4fca-b16d-55ed4d84d6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vamsl.elicitation.simulators import bernoulli_simulator, beta_simulator\n",
    "from vamsl.elicitation import edgeElicitation, graphOracle\n",
    "\n",
    "key = random.PRNGKey((seed:=195))\n",
    "# Eliciation parameters\n",
    "n_queries = 4\n",
    "random_queries = True\n",
    "stochastic = False\n",
    "expert_reliability = (0.75, 0.05, 1, 1, 1) # means = 0.75, 0.5, 0.25, vars = 0.05, 0.1, 0.2\n",
    "\n",
    "# User simulator and utility function\n",
    "#simulator, expected_utility, soft = bernoulli_simulator(), 'Rao-Blackwellized EIG', False # for hard constraints\n",
    "simulator, expected_utility, soft = beta_simulator(), 'NMC EIG', True # for soft constraints\n",
    "\n",
    "# Create query selector and oracle for simulating expert elicitation\n",
    "elicitor = edgeElicitation(simulator=simulator, expected_utility=expected_utility)\n",
    "oracle = graphOracle(ground_truth_graphs[order])\n",
    "\n",
    "E = 0.5 * jnp.ones_like(vamsl.get_E())\n",
    "#E = vamsl.get_E()\n",
    "# Initialize list of all possible experiments [n_components, n_vars**2, 2]\n",
    "experiment_lists = np.array([[(i, j) for i in range(n_vars) for j in range(n_vars) if not i==j] for k in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04ad36-0b77-4663-97a4-8ef3e40e8c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vamsl.utils import visualize_elicitation\n",
    "\n",
    "# Elicitation\n",
    "#experiment_lists = np.array([[(i, j) for i in range(n_vars) for j in range(n_vars) if not i==j] for k in range(n_components)])\n",
    "indices_list = [] # list of edges queries to remove from experiment list\n",
    "for component in range(n_components): # TODO: Make function\n",
    "    # Get component parameters which correspond to particle-wise edge probabilities\n",
    "    q_z_k, E_k = vamsl.get_posteriors()[0][component], vamsl.get_E()[component]\n",
    "    parameter_samples = jax.vmap(vamsl.edge_probs, (0, None, None))(q_z_k, 1000, E_k)\n",
    "    \n",
    "    # Get optimal queries\n",
    "    experiment_list = experiment_lists[component]\n",
    "    key, subk = random.split(key)\n",
    "    exps, EIGs, indices = elicitor.optimal_queries(parameter_samples=parameter_samples, \n",
    "                                                   experiment_list=experiment_list,\n",
    "                                                   n_queries=n_queries,\n",
    "                                                   key=subk)\n",
    "    if random_queries:\n",
    "        # overwrite BED queries with random\n",
    "        indices = np.random.choice(range(experiment_list.shape[0]), n_queries, replace=False)\n",
    "        exps = experiment_list[indices]\n",
    "    \n",
    "    # Update elicitation matrix\n",
    "    key, subk = random.split(key)\n",
    "    E = oracle.update_elicitation_matrix(E=E, component=component, queries=exps,\n",
    "                                         stochastic=stochastic, key=subk, \n",
    "                                         reliability=expert_reliability, soft=soft)\n",
    "    indices_list.append(indices)\n",
    "    \n",
    "    visualize_elicitation(mats=jnp.array([ground_truth_graphs[order][component],*[s for s in parameter_samples], E[component]]), component=component, order=order)\n",
    "\n",
    "# Remove experiments that were queried from experiment list\n",
    "experiment_lists = np.array([np.delete(exp_list, exp_is, axis=0) for exp_list, exp_is in zip(experiment_lists, indices_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff6d9b-fc8b-4a78-8d0e-c471d856c332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update the VaMSL elicitation matrix \n",
    "visualize_ground_truths(mats=E, graph_label='New elicitation matrix')\n",
    "vamsl.set_E(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d05112-fcaa-486b-b68c-e9071175b007",
   "metadata": {},
   "source": [
    "### Save model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec29b60-e2a1-4621-b055-f1ae62cfa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#filename = '5_d_14_p_100_s_30_e_2000_s.p'\n",
    "pickle.dump(vamsl.get_posteriors(), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653731a4-b242-4e84-a2e8-1996976b253e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "posts = pickle.load(open('20_d_14_p_200_s_20_e_2000_s.p', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
