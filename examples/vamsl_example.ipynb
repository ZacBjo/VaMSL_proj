{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e05811-beaf-4e42-bc46-b948e97bf102",
   "metadata": {},
   "source": [
    "### VaMSL: Approximate inference of $p(G, \\Theta, C,  \\pi | D)$ for mixtures of (non)linear Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bdb043-b9d4-4778-a151-e493bdbb5291",
   "metadata": {},
   "source": [
    "VaMSL carries out approximate inference over mixtures of Bayesian networks (BN) using coordinate ascent variational inference (CAVI). We instantiate the CAVI update over BNs with the Differentiable Bayesian Structure Learning (DiBS) ([Lorch et al., 2021](https://proceedings.neurips.cc/paper/2021/hash/ca6ab34959489659f8c3776aaf1f8efd-Abstract.html)) framework and it's Stein Variational gradient descent (SVGD) ([Liu and Wang, 2016](https://proceedings.neurips.cc/paper/2016/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html)) implementation.\n",
    "\n",
    "DiBS leverages a generative model for graphs $p(G | Z)$ conditional on the continuous latent variable $Z$ to enable the use of gradient optimization methods in the (otherwise discrete) graph search accross BNs. Their SVGD implementation enables Bayesian structure learning by yielding a posterior in the BN space $p(G, \\Theta | D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49418b15-435a-490e-8fde-dbbcdbdc9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ../. \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "key=random.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98f74d-e396-45d8-8d5f-285d553e9817",
   "metadata": {},
   "source": [
    "### Define the mixture model.\n",
    "\n",
    "First we define the mixture of the observations as well as the component BNs in the mixture model. \n",
    "\n",
    "The elements of `mixing rate` defines the proportion of the total number of observations that are generated from each component. The entries should sum up to 1. The number of components `n_components` in the mixture model is taken to be the number indicated by the mixing rates, but can be set to a different value if so desired.\n",
    "\n",
    "Each Gaussain BN has `n_vars` variables and has either a linear or nonlinear mean function depending on `struct_eq_type`. The nonlinear functions are modelled using shallow NNs. The random graph prior is either set to `er` (Erdos-Renyi) or `sf` (scale-free) graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5b263-22be-4284-8a2c-ef389863b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "key = random.PRNGKey(523)\n",
    "\n",
    "# Data settings\n",
    "n_observations = 0 # total number of observations\n",
    "mixing_rate = jnp.array([1.0]) # mixing rate of observations (also defines number of components)\n",
    "#mixing_rate = 1/(K:=5) * jnp.ones(K) # uniform mixing rates for K components\n",
    "\n",
    "# BN settings\n",
    "n_vars = 3 # number of variables in each component BN\n",
    "struct_eq_type = 'linear' # BN function class: 'linear' or 'nonlinear'\n",
    "graph_type = 'sf' # Random graph structure: 'sf' (scale-free) or 'er' (Erdos-Renyi)\n",
    "\n",
    "# Derived variables\n",
    "n_components = mixing_rate.shape[0]\n",
    "linear = True if struct_eq_type == 'linear' else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec847fcc-3cd2-4a8f-a0ff-3e049fdccb66",
   "metadata": {},
   "source": [
    "### Generate ground truth BNs and synthetic data.\n",
    "\n",
    "We generate synthetic data from ground truth BNs as specified above. `x_ind` is an observation array with {n_observations, n_vars+1}, where the last column consits of ground truth indicators for the corresponding ground truth BN. The varaibles `lik`, `component_lik`, and `graph_model` define likelihood models and the grpah prior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e590ef-cc46-4001-9348-311fc26760ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vamsl.target import make_mixture_model\n",
    "from vamsl.utils import visualize_ground_truths\n",
    "\n",
    "key, subk = random.split(key)\n",
    "# Generate data (with indicators for component assignment), ground truth BNs, likelihood models, and random graph model\n",
    "x_ind, ground_truth_graphs, ground_truth_thetas, lik, component_lik, graph_model = make_mixture_model(key=subk,\n",
    "                                                                                                      mixing_rate=mixing_rate,\n",
    "                                                                                                      n_vars=n_vars,\n",
    "                                                                                                      n_observations=n_observations,\n",
    "                                                                                                      graph_type=graph_type,\n",
    "                                                                                                      struct_eq_type=struct_eq_type)\n",
    "#visualize_ground_truths(ground_truth_graphs)\n",
    "print(ground_truth_graphs)\n",
    "# Remove indicator vector from data\n",
    "x = x_ind[:,0:n_vars] # [n_observations, n_vars]\n",
    "print('Observations shape:')\n",
    "print('x: ' + str(x.shape))\n",
    "print('Indicators and counts:')\n",
    "print(jnp.unique(x_ind[:,n_vars], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613dbe4-1b5d-4bc7-b58e-1fd5e864a620",
   "metadata": {},
   "source": [
    "### Create VaMSL.\n",
    "\n",
    "We create the VaMSL model and intilitialize it with `n_particles` randomly sampled particles latent $Z$ and $\\Theta $ particles as well as uniform distributions over responsibilities and weights. If one wants to specify specific prior responsibilities and weights, they can be fed to `initialize_posteriors` as named variables `init_q_c` and `alphas`, of shapes `(n_observations, n_components)` and `(n_components,)` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fe0cf-1ac8-4cbe-8d0e-37432c3c2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vamsl.inference import VaMSL\n",
    "\n",
    "# Create VaMSL and initialize posteriors (remove indicator vecor from dataset)\n",
    "vamsl = VaMSL(x=x, graph_model=graph_model, mixture_likelihood_model=lik, component_likelihood_model=component_lik)\n",
    "key, subk = random.split(key)\n",
    "vamsl.initialize_posteriors(key=subk, n_components=n_components, n_particles=30, linear=linear)\n",
    "\n",
    "print('Posterior shapes:')\n",
    "posts = vamsl.get_posteriors()\n",
    "print('q_z:     ' + str(posts[0].shape)) # [n_components, n_particls, d, l, 2]\n",
    "print('q_theta: ' + str(posts[1].shape)) if linear else print('q_theta: ' + str(len(posts[1]))) # leading dim of n_components\n",
    "print('log_q_c: ' + str(posts[2].shape)) # [n_observations, n_components]\n",
    "print('q_pi:    ' + str(posts[3].shape)) # [n_components,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acfd37-30ff-4af4-8a61-ff07b10737f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subk = random.split(key)\n",
    "_, false_graphs, _, _, _, _ = make_mixture_model(key=subk,mixing_rate=mixing_rate,n_vars=n_vars,n_observations=n_observations,\n",
    "                                                 graph_type=graph_type,struct_eq_type=struct_eq_type)\n",
    "\n",
    "#E = jnp.ones((n_components, n_vars, n_vars)).at[i,j,...].set(1.0) # supply individual constraints \n",
    "E = jnp.where(ground_truth_graphs, 0.99, 0.01) # supply correct soft constraints \n",
    "#E = ground_truth_graphs # supply correct hard constraints\n",
    "\n",
    "#print(false_graphs)\n",
    "vamsl.set_E(E)\n",
    "vamsl.get_E()\n",
    "#vamsl.elicitation_probs_to_hard_constraints(vamsl.get_E()) # check hard constraints encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff147d92-662d-4fd7-a359-662df8066749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAVI and SVGD vars\n",
    "n_cavi_updates, steps, callback_every = 0, 500, 100\n",
    "\n",
    "# CAVI-loop\n",
    "for cavi_update in range(n_cavi_updates):\n",
    "    key, subk = random.split(key)\n",
    "    # Optimize q(Z, \\Theta)\n",
    "    vamsl.update_particle_posteriors(key=subk, steps=steps, callback_every=callback_every,\n",
    "                                     callback=vamsl.visualize_callback(), linear=linear)\n",
    "\n",
    "    # Update to optimal q(c) and q(\\pi)\n",
    "    vamsl.update_responsibilities_and_weights()\n",
    "    print(f'CAVI update number {cavi_update+1}/{n_cavi_updates}')\n",
    "    \n",
    "# Final CAVI update with more SVGD steps to ensure annealing unto acyclic graphs\n",
    "key, subk = random.split(key)\n",
    "vamsl.update_particle_posteriors(key=subk, steps=1000, callback_every=1000, callback=vamsl.visualize_callback(), linear=linear)\n",
    "#vamsl.update_responsibilities_and_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5047901-675d-4fde-98c2-98ed805d1a8d",
   "metadata": {},
   "source": [
    "### Get order of components with respect to ground truths\n",
    "\n",
    "We identify an ordering of the components with respect to the ground truth BNs by identifying which perumutation of indeces results in the highest classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f1187-a5d7-49b2-a598-2ab6ab762905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal ordering with respect to MAP classification accuracy\n",
    "order = vamsl.identify_MAP_classification_ordering(ground_truth_indicator=x_ind[:,n_vars])\n",
    "print('Optimal order:')\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c758de4-0ab2-4208-bd54-6a90f1d85291",
   "metadata": {},
   "source": [
    "### Evaluate classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d511d-2b63-4786-a2c7-97286688c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('Sums of responsibilities:')\n",
    "print(jnp.sum(jnp.exp(vamsl.get_posteriors()[2]), axis=0))\n",
    "print('Sum of entropy of responsibilities:')\n",
    "print(jnp.sum(jnp.exp(vamsl.get_posteriors()[2])*vamsl.get_posteriors()[2]))\n",
    "y_true = x_ind[:,n_vars]\n",
    "y_pred = [order[k] for k in [jnp.argmax(c_i) for c_i in vamsl.get_posteriors()[2]]]\n",
    "if n_components==2:\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred, target_names=['Component 1', 'Component 2']))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978b293-d51f-475f-9664-cf16bab16878",
   "metadata": {},
   "source": [
    "### Evaluate structure learning performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e9849-35c1-4e70-abe4-502b4fab32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vamsl.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "# Get component datasets\n",
    "datas = [x_ind[x_ind[:,n_vars]==k][:,:n_vars] for k in range(n_components)]\n",
    "\n",
    "# Loop over components and calculate metrics\n",
    "for k, data, q_z_k, q_theta_k in zip(range(n_components), datas, vamsl.get_posteriors()[0], vamsl.get_posteriors()[1]):\n",
    "    # Get particle distribution for component\n",
    "    q_g_k = vamsl.particle_to_g_lim(q_z_k, vamsl.get_E()[order[k]])\n",
    "    dist = vamsl.get_empirical(q_g_k, q_theta_k)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    eshd = expected_shd(dist=dist, g=ground_truth_graphs[order[k]])       \n",
    "    auroc = threshold_metrics(dist=dist, g=ground_truth_graphs[order[k]])['roc_auc']\n",
    "    #negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=vamsl.eltwise_component_log_likelihood_observ, x=data.x_ho)\n",
    "    negll=0\n",
    "    \n",
    "    print(f' Component {k+1:4d} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de801d8a-0bd4-40ff-94ce-dea5c1bdca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vamsl.elicitation_probs_to_hard_constraints(vamsl.get_E())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3909cf-c46d-4a89-96ec-e19107126127",
   "metadata": {},
   "source": [
    "### Elicitation\n",
    "\n",
    "Incoporate elicited hard and soft edge constraints into VaMSL. Either provide an elicitation matrix, construct one by answering queries, or let a oracle simulate the process of querying an expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdc2b2-1eb6-4fca-b16d-55ed4d84d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vamsl.elicitation.simulators import bernoulli_simulator\n",
    "from vamsl.elicitation import edgeElicitation, graphOracle\n",
    "\n",
    "init_queries = 0 \n",
    "n_queries = 0 \n",
    "random_queries=False\n",
    "expert_reliability = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff6d9b-fc8b-4a78-8d0e-c471d856c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_1 = jnp.zeros((n_vars,n_vars)).at[0,1].set(1.0).at[0,2].set(1.0).at[2,1].set(1.0).at[1,0].set(-1.0).at[2,0].set(-1.0).at[1,2].set(-1.0)\n",
    "E_2 = jnp.zeros((n_vars,n_vars)).at[1,0].set(1.0).at[2,0].set(1.0).at[3,0].set(1.0).at[0,1].set(-1.0).at[0,2].set(-1.0).at[0,3].set(-1.0)\n",
    "visualize_ground_truth(E_1)\n",
    "visualize_ground_truth(E_2)\n",
    "#E_3 = jnp.zeros((n_vars,n_vars)).at[0,3].set(1.0).at[3,2].set(1.0).at[3,1].set(1.0)\n",
    "\n",
    "E = jnp.stack([E_1, E_2], axis=0)\n",
    "vamsl.set_E(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d05112-fcaa-486b-b68c-e9071175b007",
   "metadata": {},
   "source": [
    "### Save model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec29b60-e2a1-4621-b055-f1ae62cfa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#filename = '5_d_14_p_100_s_30_e_2000_s.p'\n",
    "pickle.dump(vamsl.get_posteriors(), open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653731a4-b242-4e84-a2e8-1996976b253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "posts = pickle.load(open('20_d_14_p_200_s_20_e_2000_s.p', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
